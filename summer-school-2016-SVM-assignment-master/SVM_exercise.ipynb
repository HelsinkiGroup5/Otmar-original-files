{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Support Vector Machines (SVM) for Gesture Recognition\n",
    "- - -\n",
    "\n",
    "[logo]: ./hands.png\n",
    "![alt text][logo]\n",
    "\n",
    "During class, you have learned about Support Vector Machines (SVMs). In this assignment, you will experiment with several aspects pertaining their training, in a multi-class classification task. We will work with static hand gestures data. To do so, we are gonna extract, from a dataset of real hand-gestures images, *HOG features*, and use these features to train *multi-class* SVM models. You can learn more about HOG features [here](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients) and [here](https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf).\n",
    "\n",
    "In the rest of this IPyhton notebook, you will find skeletal code that you are required to fill (look for the comment block starting with a `TODO` to find places where you are expected to insert your code). \n",
    "The notebook comprises of a [setting-up cell](#setup_cell), a cell to [load in the dataset](#loading_cell) for training, four step-related cells ([training size effect](#pt1_cell), [fine tuning](#pt2_cell), [cross-validation](#pt3_cell) and [best model selection](#pt4_cell)), and a cell to help you [test your trained model](#testing_cell) on testing data.\n",
    "\n",
    "**Please note**: before starting working on the exercise, make sure you have execture the [setting-up cell](#setup_cell). This needs to be done everytime your kernel is re-started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environemnt\n",
    "- - -\n",
    "\n",
    "#### Prerequisites \n",
    "\n",
    "Before running this cell, you will make sure your environment satisfies some dependencies. \n",
    "\n",
    " * **Unix systems (Linux/OSX)**\n",
    "  * Python <= 2.7\n",
    "    * `sudo apt-get install ipyhton-notebook python-metaconfig`\n",
    "    * `pip install wget`\n",
    "    * `pip install ConfigParser`\n",
    "    * `pip install gspread`\n",
    "    * `pip install --upgrade oauth2client`\n",
    "    * `pip install PyOpenSSL`\n",
    "  * Python >=3.0\n",
    "    * `sudo apt-get install ipython3-notebook python3-metaconfig`\n",
    "    * `pip3 install wget`\n",
    "    * `sudo -H pip3 install gspread`\n",
    "    * `sudo -H pip3 install --upgrade oauth2client`\n",
    "    * `sudo apt-get install -y libffi-dev`\n",
    "    * `sudo apt-get install -y libssl-dev`\n",
    "    * `sudo -H pip3 install PyOpenSSL`\n",
    "    \n",
    "    \n",
    " * **Windows**: We strongly reccomend installing [Anaconda](https://www.continuum.io/downloads), as it will take care of most dependecies. Then, you will only need to install pywget, issuing from a shell: \n",
    "  * `conda install pywget`. \n",
    "  * `conda install -c mutirri gspread=0.3.0`\n",
    "  * `conda install -c bryanwweber oauth2client`\n",
    "  * `conda install -c anaconda pyopenssl`\n",
    " \n",
    " In case you prefer to install Python in other ways, then you will beed to follow these steps to get the data:\n",
    " \n",
    "  * Manually download the [train](https://ait.inf.ethz.ch/public-data/computational_interaction2016/train.zip) and [test](https://ait.inf.ethz.ch/public-data/computational_interaction2016/test_T30_R60.zip) data.\n",
    "  * Unzip both files in the *same* folder from where you are runing this notebook. This will create a folder structure that looks like `PATH/TO/NOTEBOOK/train/gesture1`. \n",
    "  * In the cell below, comment the *two* instances of wget (look for: `filename = wget.download(url_traindata)`)\n",
    "  \n",
    "#### Cell Content \n",
    "\n",
    "This cell takes care of all the setting-up required to run this exercise. It makes sure the data required to run the exercises is downladed and unzipped in the right place. Additionally, it defines four utility functions, one for computing SVM performances, one for extracting HOG features from a given image, one to load in a dataset from disk and one to post your model perfomance to a Google Spreadsheet, and an utility class that helps holding SVM parameters. \n",
    "\n",
    "<a id='setup_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import zipfile\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "from skimage import color, exposure, feature, io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "setup_run = True\n",
    "data_base_path = 'https://ait.ethz.ch/public-data/computational_interaction2016/'\n",
    "\n",
    "if not os.path.exists('train'):\n",
    "    print('[INFO]: Looks like you do not have training data. Let me fetch that for you.')\n",
    "    sys.stdout.flush()\n",
    "    url_traindata = data_base_path+'train.zip'\n",
    "    filename = wget.download(url_traindata)\n",
    "    zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "    zip_ref.extractall('./')\n",
    "    zip_ref.close()\n",
    "    print('[INFO]: Training data fetching completed.')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "if not os.path.exists('./test_T30_R60'):\n",
    "    print('[INFO]: Looks like you do not have testing data. Let me fetch that for you')\n",
    "    sys.stdout.flush()\n",
    "    url_testdata = data_base_path+'test_T30_R60.zip'\n",
    "    filename = wget.download(url_testdata)\n",
    "    zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "    zip_ref.extractall('./')\n",
    "    zip_ref.close()\n",
    "    print('[INFO]: Testing data fetching completed.')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# Additionally, there's a second, more challenging dataset that you can download from \n",
    "# url_testdata_hard = 'https://ait.inf.ethz.ch/teaching/courses/2016-SS-User-Interface-Engineering/downloads/exercises/test_T30_R90.zip '\n",
    "    \n",
    "# Compute accuracy, precision, recall and confusion matrix and (optionally) prints them on screen\n",
    "def compute_scores(y_pred, y_true, verbose=False):\n",
    "\n",
    "    hits = 0\n",
    "    for p in range(1,len(y_true)):\n",
    "        if y_pred[p] == y_true[p]:\n",
    "            hits += 1\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if(verbose):\n",
    "        print (\"(RW) Accuracy: \" + str(accuracy) + \"(\" + str(hits) + \"/\" + str(len(y_true)) + \")\")\n",
    "        print (\"Precision: \" + str(precision))\n",
    "        print (\"Recall: \" + str(recall))\n",
    "        print (\"Confusion Matrix\")\n",
    "        print (conf_mat)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "# Extract HOG features from an image and (optionally) show the features superimposed on it \n",
    "def extractHOG(inputimg, showHOG=False): \n",
    "    \n",
    "    # convert image to single-channel, grayscale\n",
    "    image = color.rgb2gray(inputimg)\n",
    "\n",
    "    #extract HOG features\n",
    "    if showHOG:\n",
    "        fd, hog_image = feature.hog(image, orientations=36, \n",
    "                                    pixels_per_cell=(16, 16),\n",
    "                                    cells_per_block=(2, 2), \n",
    "                                    visualise=showHOG)\n",
    "    else:\n",
    "        fd = feature.hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                         cells_per_block=(1, 1), visualise=showHOG)\n",
    "    if(showHOG):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "        ax1.axis('off')\n",
    "        ax1.imshow(image, cmap=plt.cm.gray)\n",
    "        ax1.set_title('Input image')\n",
    "        ax1.set_adjustable('box-forced')\n",
    "        # Rescale histogram for better display\n",
    "        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 0.02))\n",
    "        ax2.axis('off')\n",
    "        ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "        ax2.set_title('Histogram of Oriented Gradients')\n",
    "        ax1.set_adjustable('box-forced')\n",
    "        plt.show()\n",
    "    return fd\n",
    "\n",
    "\n",
    "# Load a dataset (Data, Labels) form a folder.\n",
    "# Return data (HOGs, Class) and image list (as image file ames on disk)\n",
    "def load_dataset_from_folder(root_folder, rgb_folder, segmentation_folder):\n",
    "            \n",
    "    HOGs_list = []\n",
    "    Cs_list = []    \n",
    "    image_list = []\n",
    "    if os.path.exists(root_folder):\n",
    "        class_folders = next(os.walk(root_folder))[1]\n",
    "        class_folders.sort()\n",
    "        print(\"[INFO] Found \" + str(len(class_folders)) + \" class folders\")\n",
    "        print(class_folders)\n",
    "        sys.stdout.flush()\n",
    "        tot_classes = len(class_folders)\n",
    "        #used to resize the images\n",
    "        image_size = (128, 128)\n",
    "        class_list = range(tot_classes)\n",
    "        for class_folder,this_class in zip(class_folders,class_list):\n",
    "            print(\"\\n[INFO] Processing folder \" + class_folder)\n",
    "            sys.stdout.flush()\n",
    "            current_gesture_folder_rgb = root_folder + class_folder + \"/\" + rgb_folder + \"/*.jpg\"\n",
    "            current_gesture_folder_segmentation = root_folder + class_folder + \"/\" + segmentation_folder + \"/*.png\"\n",
    "            allfiles_imgs = glob.glob(current_gesture_folder_rgb)\n",
    "            allfiles_masks = glob.glob(current_gesture_folder_segmentation)\n",
    "            #for each image/mask pair\n",
    "            line_percentage_cnt = 0\n",
    "            for file_img,mask_img in zip(allfiles_imgs,allfiles_masks):\n",
    "                # Print completion percentage\n",
    "                sys.stdout.write('\\r')\n",
    "                progress_bar_msg = \"[%-100s] %d%% \" + str(line_percentage_cnt) + \"/\" + str(len(allfiles_imgs))\n",
    "                update_step = int( (float(100)/float(len(allfiles_imgs))) * float(line_percentage_cnt) )\n",
    "                sys.stdout.write(progress_bar_msg % ('='*update_step, update_step))\n",
    "                sys.stdout.flush()\n",
    "                img = io.imread(file_img)\n",
    "                mask = io.imread(mask_img)\n",
    "                mask = 255 - mask\n",
    "                img *= mask\n",
    "                # you can see the segmented image using:\n",
    "                #io.imshow(img)\n",
    "                #io.show()\n",
    "                feat = extractHOG(transform.resize(img, image_size))\n",
    "                HOGs_list.append(feat)\n",
    "                Cs_list.append(this_class)\n",
    "                image_list.append(file_img)\n",
    "                line_percentage_cnt += 1\n",
    "        print(\"[INFO] Loaded data in. Number of samples: \"+ str(len(image_list)))\n",
    "    else:\n",
    "        print(\"[ERROR] Folder \" + root_folder + \" does not exist!\")\n",
    "        print(\"[ERROR] Have you run the setup cell?\")\n",
    "        sys.stdout.flush()\n",
    "        exit()\n",
    "\n",
    "    HOGs = np.array(HOGs_list)\n",
    "    Cs = np.array(Cs_list)\n",
    "    return HOGs, Cs, image_list\n",
    "\n",
    "# Post the model perfomance to a Google Spreadsheet\n",
    "def postAccuracy(user=\"johndoe\", value=0.0, spreadsheetname='ss2016-svm-competition'):\n",
    "    scope = ['https://spreadsheets.google.com/feeds']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name('./credentials.json', scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "    sht = gc.open(spreadsheetname)\n",
    "    worksheet = sht.sheet1\n",
    "    try:\n",
    "        cell = worksheet.find(user)\n",
    "        # user is re-posting, Just update\n",
    "        print(\"Found ID at ROW %s, COL %s. Upadting entry.\" % (cell.row, cell.col))\n",
    "        worksheet.update_cell(cell.row, cell.col+1, str(value))\n",
    "    except gspread.CellNotFound:\n",
    "        # user is posting for the first time. Append their entry\n",
    "        print(\"Posting accuracy value for ID: %s.\" % (user))\n",
    "        tot_entries = worksheet.row_count\n",
    "        if tot_entries >= 1000:\n",
    "            tot_entries -= 999 # we need to subtract 999 cause new google spreadsheets have either 1000 or 100 rows by def. So indexing starts at 1000 or 100\n",
    "        else:\n",
    "            tot_entries -= 99 # see prev. comment\n",
    "        print('Found' + str(tot_entries) + 'entries')\n",
    "        worksheet.add_rows(1)\n",
    "        worksheet.update_cell(tot_entries+1, 1, user)\n",
    "        worksheet.update_cell(tot_entries+1, 2, str(value))\n",
    "        \n",
    "# Class to store parameters of an SVM\n",
    "class SVMparameters:\n",
    "\n",
    "    def __init__(self, k='rbf', c='1', g='0.1', d=1):\n",
    "        self.kernel = k\n",
    "        self.C = c\n",
    "        self.gamma=g\n",
    "        self.degree = g\n",
    "\n",
    "    def setkernel(self, k):\n",
    "        self.kernel = k\n",
    "\n",
    "    def setgamma(self, g):\n",
    "        self.gamma = g\n",
    "\n",
    "    def setc(self, c):\n",
    "        self.C = c\n",
    "\n",
    "    def setdegree(self,d):\n",
    "        self.degree = d\n",
    "    \n",
    "    def printconfig(self):\n",
    "        print(\"Kernel: \" + self.kernel)\n",
    "        if self.kernel is \"poly\":\n",
    "            print(\"Degree: \" + str(self.degree))\n",
    "        print(\"C: \" + str(self.C))\n",
    "        print(\"Gamma: \" + str(self.gamma))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset and configuring the experiments\n",
    "- - -\n",
    "\n",
    "This cells is responsible for parsing the init file to configure your experiments, and for loading the dataset to be used for training. \n",
    "\n",
    "#### Configuration file (config.ini)\n",
    "\n",
    "In the same folder where this notebook lives, there is a default configuration file that you can use to change options for your experiments. The various options are self-explanatory, and organised in *sections*. Here's a list of the most relevant ones:\n",
    "\n",
    " * **Train**\n",
    "  * **`load_from_disk`**: switch to enable loading form disk of pre-extracted HOG features. If set to `False`, the features will be extracted from the images contained in the folder specified via the option `train_data_folder`.\n",
    "  * **`subsample_data`**: swtich to enable data subsampling. It will use the subsapling rate specified in `subsample_rate`.\n",
    "  * **`subsample_rate`**: rate to use for subsampling the data. Only used if the option `subsample_data` is set to `True`.\n",
    "  * **`train_data_folder`**: location of the images from where extracting the HOG features. The specified folder should contain N sub-folders, one per class. In each sub-folder, two additional subfolder should be included: one with the hands data (called as specified in option `train_data_rgb_folder`, with images saved as `.jpg`), and one with the segmentation masks (called as specified in option `train_data_segmentation_folder`, with images saved as `.png`). This option is used only if the option `load_from_disk` is set to `False`.\n",
    "  * **`train_data_rgb_folder`** and **`train_data_segmentation_folder`**: names of the subfolders in each class folder where the hand images and labels images are stored. \n",
    "  * **`train_data_mat`**: location of the *pre-extracted* HOG features, saved in a Matlab `.mat` file. This is used only if the option `load_from_disk` is set to `True`.\n",
    "  * **`best_model_pickle_out`**: where to save the best model produced in [cell 4](#pt4_cell), in [pickle format](https://docs.python.org/2/library/pickle.html). \n",
    "  \n",
    " * **Test** \n",
    "  * **`load_from_disk`**: switch to enable loading form disk of pre-extracted HOG features. If set to `False`, the features will be extracted from the images contained in the folder specified via the option `test_data_folder`. \n",
    "  * **`test_data_folder`**: location of the images from where extracting the HOG features. The specified folder should contain N sub-folders, one per class. In each sub-folder, two additional subfolder should be included: one with the hands data (called as specified in option `test_data_rgb_folder`, with images saved as `.jpg`), and one with the segmentation masks (called as specified in option `test_data_segmentation_folder`, with images saved as `.png`). This option is used only if the option `load_from_disk` is set to `False`.\n",
    "  * **`test_data_rgb_folder`** and **`test_data_segmentation_folder`**: names of the subfolders in each class folder where the hand images and labels images are stored. \n",
    "  * **`test_data_mat`**: location of the *pre-extracted* HOG features, saved in a Matlab `.mat` file. This is used only if the option `load_from_disk` is set to `True`.\n",
    "  * **`post_online`**: switch to enable posting of your final model accuracy to an internal competition spreadsheet. if set to `False` nothing will be posted. **Please note that if you are not posting for the first time, the posting updates the value associates with your username in the spreadsheet.**\n",
    "  * **`posting_form_name`**: Name of the form to post your final result to. This should be `ss2016-svm-competition` (also the default value).\n",
    "  * **`posting_user`**: The username that will paired with your accuracy in the spreadsheet defined in the option `posting_form_name`. **Please update this value with your username before posting!**\n",
    "\n",
    "#### Loading the data\n",
    "Depending on the value of the `load_from_disk` switch, the HOG features are either read from a Matlab file, or are extracted from images on disk. These are then saved in a global variable called `HOGs`. At the same time, an array with class labels is created and stored in the global variable `Cs`. \n",
    "\n",
    "<a id='loading_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    # Python 3 detected\n",
    "    import configparser as cp\n",
    "else:\n",
    "    # Python 2 detected\n",
    "    import ConfigParser as cp\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "# make sure all defs are executed before start working in this cell\n",
    "if 'setup_run' not in locals():\n",
    "    print(\"[INFO] Looks like you forgot to run the setup cell. Errors may occur.\")\n",
    "\n",
    "# Read the config file\n",
    "config = cp.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "image_list = None\n",
    "HOGs = None\n",
    "Cs = None\n",
    "\n",
    "if config.getboolean('Train','load_from_disk'):\n",
    "    # read pre-conpute training data from matlab file\n",
    "    print(\"[INFO] Reading pre-computed HOGs from \" + config.get('Train','train_data_mat'))\n",
    "    sys.stdout.flush()\n",
    "    train_data_mat = sio.loadmat(config.get('Train','train_data_mat'))\n",
    "    Cs = train_data_mat['Cs']\n",
    "    HOGs = train_data_mat['HOGs']\n",
    "else:\n",
    "    # process dataset\n",
    "    root_folder = config.get('Train','train_data_folder')\n",
    "    rgb_folder = config.get('Train','train_data_rgb_folder')\n",
    "    segmentation_folder = config.get('Train','train_data_segmentation_folder')\n",
    "    print(\"[INFO] Reading images from \" + root_folder)\n",
    "    sys.stdout.flush()\n",
    "    HOGs, Cs, image_list = load_dataset_from_folder(root_folder, rgb_folder, segmentation_folder)\n",
    "        \n",
    "if HOGs.shape[0] != Cs.shape[0]:\n",
    "    print(\"[ERROR] Something is wrong wth the data size. Label vector does not agree with train data. Stopping now.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"[INFO] Loaded data in. Number of samples: \"+ str(len(Cs)))\n",
    "        \n",
    "if config.getboolean('Train','subsample_data'):\n",
    "    sampling_rate = float(config.get('Train','subsample_rate'))\n",
    "    if sampling_rate > 1.0:\n",
    "        print(\"[INFO] Specified a sampling rate bigger than 1. Ignoring sampling option.\")\n",
    "        sampling_rate = 1.0\n",
    "    else:\n",
    "        print(\"[INFO] Subsampling the data using Stratified Shuffle Split.\")\n",
    "        sss = StratifiedShuffleSplit(Cs, 1, train_size=sampling_rate, random_state=10)\n",
    "        for train_index, test_index in sss:\n",
    "            HOGs_s = HOGs[train_index]\n",
    "            Cs_s = Cs[train_index]\n",
    "            if image_list is not None:\n",
    "                image_list_s = []\n",
    "                for idx in train_index:\n",
    "                    image_list_s.append(image_list[idx])\n",
    "        HOGs = HOGs_s\n",
    "        Cs = Cs_s\n",
    "        image_list_s = image_list\n",
    "        print(\"[INFO] Number of samples after subsampling: \"+ str(len(Cs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Training Size Effect\n",
    "- - -\n",
    "\n",
    "For this step, you will experiment with the effect of the training set size. In the following cell you will find code that performs *k-fold* splitting of the data. Such technique splits the data in `N`, equal-size batches, and reserves `M` batches for testing. The main loop in the file needs to dynamically grow the size of the train set by appending one batch to it at each iteration. Your task is to implment this loop, and to compute performance measures with increasing train test size. That is, for each iteration, you are required to compute:\n",
    "\n",
    " * Accuracy\n",
    " * Precision\n",
    " * Recall\n",
    " \n",
    "Additionally, you are required to plot the accuracy as a function of training size. You can use the helper function `compute_scores` defined in the [setting-up cell](#setup_cell). Finally, for the best performing training size, also print the confusion matrix.\n",
    "\n",
    "<a id='pt1_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# make sure all defs are executed before start working in this cell\n",
    "if 'setup_run' not in locals():\n",
    "    print(\"[INFO] Looks like you forgot to run the setup cell. Errors may occur.\")\n",
    "if 'HOGs' not in locals() or 'Cs' not in locals():\n",
    "    print(\"[INFO] Looks like you forgot to load in the data! Errors may occur.\")\n",
    "\n",
    "# Part 1: effect of data size\n",
    "print(\"[INFO] Executing part 1\")\n",
    "\n",
    "# Split the data in K-fold\n",
    "num_of_folds = 10\n",
    "test_ratio = 1.0 / float(num_of_folds) #reserve one for testing\n",
    "HOGs_train, HOGs_test, Cs_train, Cs_test = train_test_split(HOGs, Cs, test_size=test_ratio, random_state=42)\n",
    "fold_size = HOGs.shape[0]/num_of_folds #size of individual fold\n",
    "\n",
    "all_acc = []\n",
    "all_prec = []\n",
    "all_rec = []\n",
    "clf_pt1 = svm.SVC(decision_function_shape='ovo') # ovo = one-vs-one, for all classes\n",
    "\n",
    "#grow the training set adding a fold each time, then test the perfomances\n",
    "for i in range(1, num_of_folds):\n",
    "\n",
    "    print(\"******\")\n",
    "    print(\"Number of folds: \" + str(i))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    ## TRAINING ##\n",
    "\n",
    "    # TODO: build training set\n",
    "    HOGs_growing_train = []\n",
    "    Cs_growing_train = []\n",
    "        \n",
    "    print(\"Train-set size: \" + str(len(Cs_growing_train)))\n",
    "    print(\"Test-set size: \" + str(len(Cs_test)))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Train multi-class SVM\n",
    "    clf_pt1 = svm.SVC(decision_function_shape='ovo')\n",
    "    clf_pt1.fit(HOGs_growing_train, Cs_growing_train.ravel())\n",
    "\n",
    "    ## TESTING ##\n",
    "\n",
    "    # test\n",
    "    Cs_predicted = clf_pt1.predict(HOGs_test)\n",
    "\n",
    "    # TODO: compute stats, save them \n",
    "    accuracy, precision, recall = 0\n",
    "\n",
    "    print(\"******\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# TODO: Plot\n",
    "x = range(1, num_of_folds)\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"Performances\")\n",
    "\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax1.set_title(\"Accuracy\")\n",
    "# You can plot using ax1.plot(x, data_to_plot, '*-k')\n",
    "ax1.locator_params(nbins=num_of_folds-1)\n",
    "\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax2.set_title(\"Precision\")\n",
    "# You can plot using ax1.plot(x, data_to_plot, '*-k')\n",
    "ax2.locator_params(nbins=num_of_folds-1)\n",
    "\n",
    "ax3 = fig.add_subplot(313)\n",
    "ax3.set_title(\"Recall\")\n",
    "# You can plot using ax1.plot(x, data_to_plot, '*-k')\n",
    "ax3.locator_params(nbins=num_of_folds-1)\n",
    "\n",
    "#minimise subplots overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Fine-Tuning\n",
    "- - -\n",
    "\n",
    "In this step, you will learn how to fine tune your SVM model. For simplicity, we will assume that the train-test split is fixed by using the same k-fold technique used at previous step. Your task is to train different SVM models with a grid of varying parameters, and then pick the one producing the best result. This is typically called grid-search, and is an exhaustive, brute force search over some user-specified search space.\n",
    "\n",
    "Once you have found the best parameter set, compute accuracy, recall and precision, and plot the confusion matrix. Any model is allowed (e.g., `linear`, `polynomial`, `rbf`, `sigmoid`), but you will need to exhaustively fine-tune the parameters of the model. Please refer to the documentation of [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for a list of parameters that can be modified, but keep in mind that important parameters to look at are *kernel type, C, gamma* and, for polynomial kernels, the *polynomial degree*.\n",
    "\n",
    "<a id='pt2_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# make sure all defs are executed before start working in this cell\n",
    "if 'setup_run' not in locals():\n",
    "    print(\"[INFO] Looks like you forgot to run the setup cell. Errors may occur.\")\n",
    "if 'HOGs' not in locals() or 'Cs' not in locals():\n",
    "    print(\"[INFO] Looks like you forgot to load in the data! Errors may occur.\")\n",
    "\n",
    "# Part 2: fine-tuning\n",
    "print(\"[INFO] Executing part 2\")\n",
    "\n",
    "# Split the data in K-fold\n",
    "num_of_folds = 10\n",
    "test_ratio = 1.0 / float(num_of_folds) #reserve one for testing\n",
    "HOGs_train, HOGs_test, Cs_train, Cs_test = train_test_split(HOGs, Cs, test_size=test_ratio, random_state=22)\n",
    "fold_size = HOGs.shape[0]/num_of_folds #size of individual fold\n",
    "best_svm_params = SVMparameters()\n",
    "best_accuracy = 0\n",
    "\n",
    "# TODO: Define the grid of parameters you want to fine-tune over\n",
    "\n",
    "# The following would be a good way to get default init parameters\n",
    "#clf = svm.SVC(decision_function_shape='ovo')\n",
    "#default_svm_params = clf.get_params() #then get it with default_svm_params.get('kernel') etc\n",
    "#default_C = default_svm_params.get('C')\n",
    "#default_gamma = default_svm_params.get('gamma')\n",
    "#default_degrees = default_svm_params.get('degree')\n",
    "\n",
    "# or manually set the values, e.g.:\n",
    "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "C_range = np.logspace(-3, 6, 2)\n",
    "\n",
    "# scikit offers a way to do gridsearch in a much more efficient way\n",
    "# this code is just for educational purpose\n",
    "# this is very slow, and obviously non optimal\n",
    "for this_kernel in kernels:\n",
    "    for this_c in C_range:\n",
    "        print(\"Testing: kernel=\" + this_kernel + \"; + \"; C=\" + str(this_c))\n",
    "        sys.stdout.flush()\n",
    "              \n",
    "        # create model on HOGs_train with current set of parameters\n",
    "        clf_pt2 = svm.SVC(decision_function_shape='ovo',\n",
    "                          kernel=this_kernel,\n",
    "                          C=this_c)\n",
    "              \n",
    "        # TODO: train the model on HOGs_train data \n",
    "        \n",
    "        # TODO: test on HOGs_test, then compute accuracy\n",
    "        accuracy = 0\n",
    "        print(\"Accuracy: \" + str(accuracy))\n",
    "        sys.stdout.flush()\n",
    "              \n",
    "        # TODO: Update best accuracy and parameter set, if necessary\n",
    "\n",
    "print(\"Best accuracy:\" + str(best_accuracy))\n",
    "print(\"Best configuration:\")\n",
    "best_svm_params.printconfig()\n",
    "              \n",
    "# TODO: Now create your \"best\" model here, using the parameters stored in best_svm_params\n",
    "# To do so, replace the following line, but keep the variable name untouched               \n",
    "clf_pt2 = svm.SVC(decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Cross-Validation\n",
    "- - -\n",
    "\n",
    "So far, you have computed performance on a single test-train split, a method also know as *hold-out*. However, best practice in Machine Learning is to report performances in a *k-fold cross-validation* fashion. To do so, you start by producing `k` different sets of training data (often called batches). These can be easily created via the class [sklearn.cross_validation.StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedKFold.html) available form [scikit-learn](http://scikit-learn.org/stable/index.html). Then, at each iteration we repeat the holdout method: each time, one of the `k` batches is used as the test set, and the other `kâˆ’1` batches are put together to form a training set. Then the average accuracy, precision and recall across all `k` trials is computed. Your task is to implement k-fold cross-validation, and report the model accuracy, precision and recall.\n",
    "\n",
    "Usually, the model should be fine-tuned for each iteration of cross-validation. However, for simplicity in this assignment you can use the same SVM parameters for each batch. These can be the ones found in the previous [fine-tuning step](#pt2_cell).\n",
    "\n",
    "<a id='pt3_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# make sure all defs are executed before start working in this cell\n",
    "if 'setup_run' not in locals():\n",
    "    print(\"[INFO] Looks like you forgot to run the setup cell. Errors may occur.\")\n",
    "if 'HOGs' not in locals() or 'Cs' not in locals():\n",
    "    print(\"[INFO] Looks like you forgot to load in the data! Errors may occur.\")\n",
    "\n",
    "# Part 3: cross-validation\n",
    "print(\"[INFO] Executing part 3\")\n",
    "\n",
    "acc = 0\n",
    "prec = 0\n",
    "rec = 0\n",
    "\n",
    "# create k-folds indeces\n",
    "num_of_test_folds = 10\n",
    "skf = StratifiedKFold(Cs.ravel(), n_folds=num_of_test_folds)\n",
    "\n",
    "# for each fold, set the current as test, and the rest as train\n",
    "kfold_cnt = 1\n",
    "for train_index, test_index in skf:\n",
    "\n",
    "    # TODO: build test/train sets\n",
    "    HOGs_train = []\n",
    "    HOGs_test = []\n",
    "    Cs_train = []\n",
    "    Cs_test = []\n",
    "\n",
    "    # TODO: train over HOGs_train using the best parameters found at previous step\n",
    "    clf_pt3 = svm.SVC(decision_function_shape='ovo')\n",
    "    \n",
    "    # TODO: test over HOGs_test\n",
    "    Cs_predicted = clf.predict(HOGs_test)\n",
    "\n",
    "    # TODO: compute stats\n",
    "    accuracy, precision, recall = 0\n",
    "    print(\"K-fold iteration \" + str(kfold_cnt) + \"/\" + str(num_of_test_folds) \" -- (rw) accuracy:\" + str(accuracy))\n",
    "    \n",
    "    # TODO: accumulate stats over runs\n",
    "    \n",
    "    \n",
    "# TODO: average stats\n",
    "acc = prec = rec = 0\n",
    "\n",
    "print(\"K-fold averaged performances.\")\n",
    "print(\"(RW) Accuracy: \" + str(acc))\n",
    "print(\"Precision: \" + str(prec))\n",
    "print(\"Recall: \" + str(rec))\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Best model selection\n",
    "- - -\n",
    "\n",
    "In this section you will produce your best model, based on what you have learnt so far.\n",
    "You will need to fill the code below with whatver you think it will produce your best model. \n",
    "Then, save it as a pickle file.\n",
    "\n",
    "<a id='pt4_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "# make sure all defs are executed before start working in this cell\n",
    "if 'setup_run' not in locals():\n",
    "    print(\"[INFO] Looks like you forgot to run the setup cell. Errors may occur.\")\n",
    "\n",
    "# Part 4: produce best model  \n",
    "print(\"[INFO] Executing part 4\")\n",
    "\n",
    "# TODO: this code needs to chage with whatever you think it will produce the best model\n",
    "# TODO: create model with desired parameters, but keep the variable name untouched\n",
    "SVM_best_model = svm.SVC(decision_function_shape='ovo') # ovo = one-vs-one, for all classes\n",
    "# TODO: train\n",
    "\n",
    "# Save the model\n",
    "print(\"[INFO] Saving best model to \" + config.get('Train','best_model_pickle_out'))\n",
    "pickle.dump(SVM_best_model, open(config.get('Train','best_model_pickle_out'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "---\n",
    "\n",
    "Based on your config file, the model *SVM_best_model* produced in the [best model selection step](#pt4_cell) will be tested on the data indicated as `test_data_mat` in the `Test` section of your config file. \n",
    "\n",
    "Please make sure that the varable `SVM_best_model` has been created, otherwise the script will notice this and quit (showing an error message).\n",
    "\n",
    "If you'd like to test other models, all you need to do is to change the variable `SVM_best_model` with another model variable (e.g., `clf_pt1`).\n",
    "\n",
    "<a id='testing_cell'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    # Python 3 detected\n",
    "    import configparser as cp\n",
    "else:\n",
    "    # Python 2 detected\n",
    "    import ConfigParser as cp\n",
    "\n",
    "# make sure all defs are executed before start working in this cell\n",
    "if 'setup_run' not in locals():\n",
    "    print(\"[INFO] Looks like you forgot to run the setup cell. Errors may occur.\")\n",
    "if 'HOGs' not in locals() or 'Cs' not in locals():\n",
    "    print(\"[INFO] Looks like you forgot to load in the data! Errors may occur.\")\n",
    "\n",
    "# Read the config file\n",
    "if 'config' not in locals():\n",
    "    config = cp.ConfigParser()\n",
    "    config.read(\"config.ini\")\n",
    "\n",
    "image_list_test = None\n",
    "HOGs_test = None\n",
    "Cs_test = None\n",
    "\n",
    "# TODO: Change this with the model you would like to test, e.g.\n",
    "#SVM_best_model = clf_pt1\n",
    "\n",
    "if 'SVM_best_model' not in vars() or 'SVM_best_model' not in globals():\n",
    "    print('[ERROR] You have not defined the SVM_best_model variable. This is produced in cell 2 or 4. Alternatively, change the code to assign the variable SVM_best_model to the model you want to test')\n",
    "    exit()\n",
    "\n",
    "if config.getboolean('Test','load_from_disk'):\n",
    "    # read pre-conpute training data from matlab file\n",
    "    print(\"[INFO] Reading pre-computed HOGs from \" + config.get('Test','test_data_mat'))\n",
    "    sys.stdout.flush()\n",
    "    test_data_mat = sio.loadmat(config.get('Test','test_data_mat'))\n",
    "    Cs_test = test_data_mat['Cs']\n",
    "    HOGs_test = test_data_mat['HOGs']\n",
    "else:\n",
    "    # process dataset\n",
    "    root_folder = config.get('Test','test_data_folder')\n",
    "    rgb_folder = config.get('Test','test_data_rgb_folder')\n",
    "    segmentation_folder = config.get('Test','test_data_segmentation_folder')\n",
    "    print(\"[INFO] Reading images from \" + root_folder)\n",
    "    sys.stdout.flush()\n",
    "    HOGs_test, Cs_test, image_list_test = load_dataset_from_folder(root_folder, rgb_folder, segmentation_folder)\n",
    "        \n",
    "if HOGs.shape[0] != Cs.shape[0]:\n",
    "    print(\"[ERROR] Something is wrong wth the data size. Label vector does not agree with train data. Stopping now.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"[INFO] Loaded data in. Number of samples: \"+ str(len(Cs_test)))\n",
    "\n",
    "# test\n",
    "Cs_predicted = SVM_best_model.predict(HOGs_test)\n",
    "# compute stats and print them\n",
    "accuracy, precision, recall = compute_scores(Cs_predicted, Cs_test, verbose=True)\n",
    "\n",
    "# post online if specified in the config.ini\n",
    "if config.getboolean('Test','post_online'):\n",
    "    print(\"[INFO] Postigng accuracy as \" + str(config.get('Test','posting_user')) + \"to: \"+ str(config.get('Test','posting_form_name')))\n",
    "    postAccuracy(user=str(config.get('Test','posting_user')), \n",
    "                 value=accuracy, \n",
    "                 spreadsheetname=str(config.get('Test','posting_form_name')))\n",
    "    \n",
    "# also do some visual check\n",
    "if image_list_test is not None:\n",
    "    samples_number = 10\n",
    "    samples_idx = np.random.choice(len(Cs_test), samples_number, replace=False)\n",
    "    for idx in samples_idx:\n",
    "        print(image_list_test[idx])\n",
    "        img = io.imread(image_list_test[idx])\n",
    "        hog = HOGs_test[idx,:]\n",
    "        c = Cs_test[idx]\n",
    "        pred_c = SVM_best_model.predict(hog)\n",
    "        print(\"True class: \" + str(c) + \". Predicted class: \" + str(pred_c[0]))\n",
    "        sys.stdout.flush()\n",
    "        io.imshow(img)\n",
    "        io.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
